### 1 监督学习、无监督学习

---

### 2 代价函数、梯度下降介绍

---

### 3 线代基础

---

### 4 多元线性回归

#### 4.2 多元线性回归梯度下降

$$
\theta _{j}:=\theta _{j}-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta }(x^{(i)})-y^{(i)})x_{j}^{(i)}
$$

其中，m是样本数，xj表示第j个特征。

注：要保证θ同时更新（simultaneous updates）

#### 4.3 4.4 影响梯度下降的因素(特征缩放、学习率)

特征缩放（feature scaling）：加快梯度下降

特征缩放的四种方式：

1. Min-Max Normalization（min-max标准化）
   $$
   x^{*}= \frac{x-min(x)}{max(x)-min(x)}
   $$

2. Mean normalization（mean归一化）
   $$
   x^{*}=\frac{x-mean(x)}{max(x)-min(x)}
   $$
   
3. Standarddization（z-score标准化）
   $$
   x^{*}=\frac{x-\bar{x}}{\sigma }
   $$
   
4. max标准化
   $$
   x^{*}=\frac{x}{max(x)}
   $$
   

学习率α：学习率α过大使得损失函数在最小值周围跳跃甚至发散，过小使得损失函数收敛过慢

#### 4.6 正规方程（区别于迭代方法的直接解析法）

正规方程（normal equation）适用于特征数较少的**线性回归**问题，梯度下降适用于线性回归和逻辑回归等

- 由正规方程，当
  $$
  \theta =(X^{T}X)^{-1}X^{T}y
  $$
  成立时，损失函数取最小值

- 当矩阵XTX不可逆时，原因是特征之间线性相关，可以删除一些特征或者正则化（regulization）

---

### 5 矩阵、向量基础的octave、matlab实现

#### 5.1-5.5 线代基础跳过

#### 5.6 矢量化

把方程组写成矩阵形式，方程组里的一系列x随即变成了向量形式。

---

### 6 逻辑回归

#### 6.1 逻辑回归（logistic regression）

logistic regression是一种分类算法，通常用于分类问题。名字里带有“回归”，历史原因造成，不要困惑。

#### 6.2 假设陈述

逻辑回归中，要使假设函数值位于[0, 1]之间，令：
$$
h_{\theta }(x)=g(\theta ^{T}x)
$$
g函数即为神经网络里的**激活函数**，有Sigmoid（1/（1+e^-x））（也叫logistic function（逻辑函数），同义词）、Tanh（(e^x-e^-x)/(e^x+e^-x)）、ReLU（max（0，x））函数

逻辑回归中，h函数的输出值的意义：
$$
h_{\theta }(x)=g(\theta ^{T}x)=P(y=1|x;\theta )
$$

#### 6.3 决策界限

强调：决策边界不是训练集的属性（不随训练集变化而变化），而是假设函数本身及其参数的属性。训练集用于修正θ参数。一旦有了参数θ，它就确定了决策边界。

#### 6.4 代价函数（如何自动拟合假设函数？）

对于线性回归，有：
$$
J(\theta )=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
$$
若令：
$$
Cost(h_{\theta}(x^{(i)}),y^{(i)})=\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
$$
在**逻辑回归**中使用该cost函数，往往会使得J(θ)是一个非凸函数（上凸--凸，下凸--凹，都叫凸函数），梯度下降可能会收敛到多个局部最优值。

所以我们令：
$$
Cost(h_{\theta}(x),y)=\left\{\begin{matrix}-log(h_{\theta}(x))
 &if  &y= 1\\ -log(1-h_{\theta}(x))
 &if  &y=0 
\end{matrix}\right.
$$
可使得J(θ)是一个凸函数，梯度下降会收敛到全局最优点。

该式由极大似然估计得出。

这个函数在y=1时，若h函数预测值靠近0，则cost函数将很大，导致损失函数很大，反映为惩罚较大，y=0时同理。

#### 6.5 简化代价函数与梯度下降

由6.4简化得出逻辑回归的cost函数：
$$
Cost(h_{\theta }(x),y)=-ylog(h_{\theta }(x))-(1-y)log(1-h_{\theta }(x))
$$
于是，对应的损失函数J(θ)：
$$
J(\theta )=\frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta }(x^{(i)}),y^{(i)})=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}log(h_{\theta }(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta }(x^{(i)}))]
$$
若h函数使用sigmoid函数来约束区间为[0,1]，则推出的梯度下降的更新规则的式子和线性回归中的一模一样。

#### 6.6 高级优化（梯度下降之外的方法）

给定θ，要更快计算J(θ)和J(θ)对θj的偏导，可使用：

- Conjugate gradient（共轭梯度法）
- BFGS
- L-BFGS

Advantages：无需手动选择学习率α、收敛速度通常快于梯度下降

Disadvantages：算法更复杂

实际选择算法时，根据表现情况来选择最优算法。

#### 6.7 逻辑回归解决多类别分类问题

三分类问题可以划分为三个独立的二分类问题。对于每种独立的情况，新建一个”伪“训练集，比如可将class2、3设定为负类，class1设定为正类。

图例：

<img src="C:\Users\45297\Desktop\111.png"  />

于是，拟合得到三个分类器h1、h2、h3函数：

![](C:\Users\45297\Desktop\333.png)

三个分类器分别训练，取：
$$
maxh_{\theta }^{(i)}(x)
$$
我们预测y就是h值最大对应的那个值。比如，若三角形的h函数（P(y=1|x;θ)）最大，则预测y=class1

---

### 7 正则化（regularization）

#### 7.1 过拟合（overfitting）问题

模型泛化能力：一个假设模型应用到新样本的能力。

解决过拟合的方法：

![](C:\Users\45297\Desktop\222.png)

#### 7.2 代价函数

正则化的思想：为了减少x3、x4两个特征带来的影响，给损失函数的参数θ3^2、θ4^2添加惩罚项1000（1000是随意添加的一个较大的数），为了使损失函数达到最小值，θ3、θ4将接近于0。

![](C:\Users\45297\Desktop\444.png)

如下图所示，使用如下方式，给损失函数添加惩罚项。

目的有二：1、提高拟合效果 2、正则化

![](C:\Users\45297\Desktop\555.png)

如果惩罚系数λ过大，会使得除θ0之外所有的θj都接近于0，拟合曲线就接近于一条平行于x轴的曲线，导致欠拟合。所以引入高度正则化，以自动选择合适的正则化参数λ。

#### 7.3 线性回归的正则化

将梯度下降和正规方程两种算法推广到正则化的线性回归。

目标：
$$
J(\theta )=\frac{1}{2m}[\sum_{i=1}^{m}(h_{\theta }(x^{(i)})-y^{(i)})^{2}+\lambda \sum_{j=1}^{n}\theta _{j}^{2}]
\\
\underset{\theta }{min}J(\theta )
$$
梯度下降：

![](C:\Users\45297\Desktop\666.png)

正规方程：

![](C:\Users\45297\Desktop\777.png)

